{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_finished_and_clean(1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {
        "id": "J0X4AR4dR5QI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Test data from our github repo\n",
        "url_test = 'https://raw.githubusercontent.com/tjayada/iANNwTF_Project/main/data/raw%20data/Test.csv'\n",
        "dataDF_test = pd.read_csv(url_test)\n",
        "\n",
        "# Load Train data from our github repo\n",
        "url_train = 'https://raw.githubusercontent.com/tjayada/iANNwTF_Project/main/data/raw%20data/Train.csv'\n",
        "dataDF_train = pd.read_csv(url_train)"
      ],
      "metadata": {
        "id": "j76gJVyhSClt"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data without clomumns 70 - 76 and drop NaN's\n",
        "\n",
        "# for test date\n",
        "data_without_columns_test = dataDF_test.copy()\n",
        "data_without_columns_test = data_without_columns_test.drop([\"L3_CH4_CH4_column_volume_mixing_ratio_dry_air\", \"L3_CH4_aerosol_height\", \"L3_CH4_aerosol_optical_depth\",\n",
        "                                      \"L3_CH4_sensor_azimuth_angle\", \"L3_CH4_sensor_zenith_angle\", \"L3_CH4_solar_azimuth_angle\", \"L3_CH4_solar_zenith_angle\"], axis=1)\n",
        "data_without_columns_test = data_without_columns_test.dropna()\n",
        "\n",
        "# for train data\n",
        "data_without_columns_train = dataDF_train.copy()\n",
        "data_without_columns_train = data_without_columns_train.drop([\"L3_CH4_CH4_column_volume_mixing_ratio_dry_air\", \"L3_CH4_aerosol_height\", \"L3_CH4_aerosol_optical_depth\",\n",
        "                                      \"L3_CH4_sensor_azimuth_angle\", \"L3_CH4_sensor_zenith_angle\", \"L3_CH4_solar_azimuth_angle\", \"L3_CH4_solar_zenith_angle\"], axis=1)\n",
        "data_without_columns_train = data_without_columns_train.dropna()"
      ],
      "metadata": {
        "id": "VWL05QVnSHBw"
      },
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove targets and create labels\n",
        "data_without_columns_train_labels = data_without_columns_train[['target']]\n",
        "data_without_columns_train = data_without_columns_train.drop([\"target\", \"target_min\", \"target_max\", \"target_variance\", \"target_count\"], axis=1)"
      ],
      "metadata": {
        "id": "6OPbC1PdSpJP"
      },
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dates to datetime \n",
        "data_without_columns_test['date'] = pd.to_datetime(data_without_columns_test['Date'],format='%Y-%m-%d')\n",
        "data_without_columns_train['date'] = pd.to_datetime(data_without_columns_train['Date'],format='%Y-%m-%d')\n",
        "\n",
        "\n",
        "# add weekdays based on dates\n",
        "data_without_columns_test['weekday'] = data_without_columns_test['date'].dt.weekday\n",
        "data_without_columns_train['weekday'] = data_without_columns_train['date'].dt.weekday\n",
        "\n",
        "\n",
        "# add whether the date is a weekend or not, 0 is no and 1 is yes\n",
        "data_without_columns_test['weekend'] = np.multiply((data_without_columns_test['date'].dt.weekday >= 5), 1)\n",
        "data_without_columns_train['weekend'] = np.multiply((data_without_columns_train['date'].dt.weekday >= 5),1)\n",
        "\n",
        "\n",
        "# add month based on dates\n",
        "data_without_columns_test['month'] = data_without_columns_test['date'].dt.month\n",
        "data_without_columns_train['month'] = data_without_columns_train['date'].dt.month\n",
        "\n",
        "\n",
        "# add which day out of all days in the year it is\n",
        "data_without_columns_test['dayofyear'] = data_without_columns_test['date'].dt.dayofyear\n",
        "data_without_columns_train['dayofyear'] = data_without_columns_train['date'].dt.dayofyear\n",
        "\n",
        "\n",
        "# what quarter is the date part of\n",
        "data_without_columns_test['quarter'] = data_without_columns_test['date'].dt.quarter\n",
        "data_without_columns_train['quarter'] = data_without_columns_train['date'].dt.quarter"
      ],
      "metadata": {
        "id": "3AG95mz9SweW"
      },
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop original dates in datetime and locations\n",
        "data_without_columns_train = data_without_columns_train.drop([\"Place_ID X Date\", \"Date\", \"Place_ID\", \"date\"], axis = 1)\n",
        "data_without_columns_test = data_without_columns_test.drop([\"Place_ID X Date\", \"Date\", \"Place_ID\", \"date\"], axis = 1)"
      ],
      "metadata": {
        "id": "yfN_bobFS22t"
      },
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize data\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# for test data\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data_without_columns_test)\n",
        "scaled = scaler.fit_transform(data_without_columns_test)\n",
        "data_without_columns_test = pd.DataFrame(scaled, columns=data_without_columns_test.columns)\n",
        "\n",
        "\n",
        "# for train data\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data_without_columns_train)\n",
        "scaled = scaler.fit_transform(data_without_columns_train)\n",
        "data_without_columns_train = pd.DataFrame(scaled, columns=data_without_columns_train.columns)"
      ],
      "metadata": {
        "id": "HE28VsBpT1qP"
      },
      "execution_count": 344,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find out most important features through PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# just tried around and found out that 6 features are the best match\n",
        "pca = PCA(n_components=6).fit(data_without_columns_train)\n",
        "\n",
        "# number of components, in our case 6\n",
        "number_of_components = pca.components_.shape[0]\n",
        "\n",
        "# get the index of the most important features\n",
        "most_important = []\n",
        "for component in range(number_of_components):\n",
        "  most_important.append(abs(pca.components_[component]).argmax())\n",
        "\n",
        "initial_feature_names = data_without_columns_train.columns\n",
        "# get the names of the most important features\n",
        "most_important_names = []\n",
        "for name in range(number_of_components):\n",
        "  most_important_names.append(initial_feature_names[most_important[name]])\n",
        "\n",
        "# individual features importance in descending order\n",
        "for i in range(number_of_components):\n",
        "  print(most_important_names[i] + \" explains \" + str(np.round(pca.explained_variance_ratio_[i] * 100)) + \"% of the variance \\n\")\n",
        "\n",
        "# total explainibility of all six features combined\n",
        "print(\"\\n All six features together can explain \" + str(np.round(sum(pca.explained_variance_ratio_)*100)) + \"% of the variance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDCaaEW8TcYr",
        "outputId": "bd9e4f2d-fd3d-4e77-f221-4e593f695ad4"
      },
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L3_SO2_sensor_azimuth_angle explains 29.0% of the variance \n",
            "\n",
            "L3_NO2_sensor_zenith_angle explains 17.0% of the variance \n",
            "\n",
            "L3_NO2_solar_zenith_angle explains 12.0% of the variance \n",
            "\n",
            "weekend explains 8.0% of the variance \n",
            "\n",
            "L3_NO2_solar_azimuth_angle explains 7.0% of the variance \n",
            "\n",
            "L3_CLOUD_cloud_fraction explains 5.0% of the variance \n",
            "\n",
            "\n",
            " All six features together can explain 78.0% of the variance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create new columns for previous data of most important features\n",
        "# in this case we take 1 day before into account\n",
        "\n",
        "number_of_shifts = 1\n",
        "for name in most_important_names:\n",
        "    for i in range(number_of_shifts):\n",
        "        data_without_columns_train[name+'_of_'+str(i+1)+'_day_before'] = data_without_columns_train[name].shift(i+1)\n",
        "        data_without_columns_test[name+'_of_'+str(i+1)+'_day_before'] = data_without_columns_test[name].shift(i+1)"
      ],
      "metadata": {
        "id": "lcXybxjxV4rF"
      },
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reset index, so we can use it to remove NaN rows in labels\n",
        "data_without_columns_train.index = np.arange(0, len(data_without_columns_train))\n",
        "data_without_columns_test.index = np.arange(0, len(data_without_columns_test))\n",
        "data_without_columns_train_labels.index = np.arange(0, len(data_without_columns_train_labels))\n",
        "\n",
        "# through shifting we have created NaN's once again, so we need to drop these rows\n",
        "data_without_columns_train = data_without_columns_train.dropna()\n",
        "data_without_columns_test = data_without_columns_test.dropna()\n",
        "\n",
        "# also remove them from the targets\n",
        "data_without_columns_train_labels = data_without_columns_train_labels.drop(index = np.array((range(number_of_shifts))))"
      ],
      "metadata": {
        "id": "tE6jVHC_WU1g"
      },
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the new and clean data\n",
        "data_without_columns_train.to_csv('data_without_columns_train.csv')\n",
        "data_without_columns_train_labels.to_csv('data_without_columns_train_labels.csv')\n",
        "data_without_columns_test.to_csv('data_without_columns_test.csv')"
      ],
      "metadata": {
        "id": "P4nd76xBWgE-"
      },
      "execution_count": 348,
      "outputs": []
    }
  ]
}