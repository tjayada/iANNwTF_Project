{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_finished_and_clean_LSTM(1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WyBNvH5elHy8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Test data from our github repo\n",
        "url_test = 'https://raw.githubusercontent.com/tjayada/iANNwTF_Project/main/data/raw%20data/Test.csv'\n",
        "dataDF_test = pd.read_csv(url_test)\n",
        "\n",
        "# Load Train data from our github repo\n",
        "url_train = 'https://raw.githubusercontent.com/tjayada/iANNwTF_Project/main/data/raw%20data/Train.csv'\n",
        "dataDF_train = pd.read_csv(url_train)"
      ],
      "metadata": {
        "id": "1CfOjCVblmIX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data without clomumns 70 - 76 and drop NaN's\n",
        "\n",
        "# for test data\n",
        "data_without_columns_test = dataDF_test.copy()\n",
        "data_without_columns_test = data_without_columns_test.drop([\"L3_CH4_CH4_column_volume_mixing_ratio_dry_air\", \"L3_CH4_aerosol_height\", \"L3_CH4_aerosol_optical_depth\",\n",
        "                                      \"L3_CH4_sensor_azimuth_angle\", \"L3_CH4_sensor_zenith_angle\", \"L3_CH4_solar_azimuth_angle\", \"L3_CH4_solar_zenith_angle\"], axis=1)\n",
        "data_without_columns_test = data_without_columns_test.dropna()\n",
        "\n",
        "# for train data\n",
        "data_without_columns_train = dataDF_train.copy()\n",
        "data_without_columns_train = data_without_columns_train.drop([\"L3_CH4_CH4_column_volume_mixing_ratio_dry_air\", \"L3_CH4_aerosol_height\", \"L3_CH4_aerosol_optical_depth\",\n",
        "                                      \"L3_CH4_sensor_azimuth_angle\", \"L3_CH4_sensor_zenith_angle\", \"L3_CH4_solar_azimuth_angle\", \"L3_CH4_solar_zenith_angle\"], axis=1)\n",
        "data_without_columns_train = data_without_columns_train.dropna()"
      ],
      "metadata": {
        "id": "G_u16WT1lqnM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove targets and create labels\n",
        "data_without_columns_train_labels = data_without_columns_train[['target']]\n",
        "data_without_columns_train = data_without_columns_train.drop([\"target\", \"target_min\", \"target_max\", \"target_variance\", \"target_count\"], axis=1)"
      ],
      "metadata": {
        "id": "b3lmTJcKlshW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dates to datetime \n",
        "data_without_columns_test['date'] = pd.to_datetime(data_without_columns_test['Date'],format='%Y-%m-%d')\n",
        "data_without_columns_train['date'] = pd.to_datetime(data_without_columns_train['Date'],format='%Y-%m-%d')\n",
        "\n",
        "\n",
        "# add weekdays based on dates\n",
        "data_without_columns_test['weekday'] = data_without_columns_test['date'].dt.weekday\n",
        "data_without_columns_train['weekday'] = data_without_columns_train['date'].dt.weekday\n",
        "\n",
        "\n",
        "# add whether the date is a weekend or not, 0 is no and 1 is yes\n",
        "data_without_columns_test['weekend'] = np.multiply((data_without_columns_test['date'].dt.weekday >= 5), 1)\n",
        "data_without_columns_train['weekend'] = np.multiply((data_without_columns_train['date'].dt.weekday >= 5),1)\n",
        "\n",
        "\n",
        "# add month based on dates\n",
        "data_without_columns_test['month'] = data_without_columns_test['date'].dt.month\n",
        "data_without_columns_train['month'] = data_without_columns_train['date'].dt.month\n",
        "\n",
        "\n",
        "# add which day out of all days in the year it is\n",
        "data_without_columns_test['dayofyear'] = data_without_columns_test['date'].dt.dayofyear\n",
        "data_without_columns_train['dayofyear'] = data_without_columns_train['date'].dt.dayofyear\n",
        "\n",
        "\n",
        "# what quarter is the date part of\n",
        "data_without_columns_test['quarter'] = data_without_columns_test['date'].dt.quarter\n",
        "data_without_columns_train['quarter'] = data_without_columns_train['date'].dt.quarter"
      ],
      "metadata": {
        "id": "M35S-CA1lxUs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create seperate data frame for place id's\n",
        "data_without_columns_train_places = data_without_columns_train[['Place_ID']]\n",
        "data_without_columns_test_places = data_without_columns_test[['Place_ID']]\n",
        "\n",
        "# drop original dates in datetime and locations\n",
        "data_without_columns_train = data_without_columns_train.drop([\"Place_ID X Date\", 'Place_ID', \"Date\", \"date\"], axis = 1)\n",
        "data_without_columns_test = data_without_columns_test.drop([\"Place_ID X Date\",'Place_ID', \"Date\", \"date\"], axis = 1)"
      ],
      "metadata": {
        "id": "ZuMW6bdOl0WV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize data\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# for test data\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data_without_columns_test)\n",
        "scaled = scaler.fit_transform(data_without_columns_test)\n",
        "data_without_columns_test = pd.DataFrame(scaled, columns=data_without_columns_test.columns)\n",
        "\n",
        "\n",
        "# for train data\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data_without_columns_train)\n",
        "scaled = scaler.fit_transform(data_without_columns_train)\n",
        "data_without_columns_train = pd.DataFrame(scaled, columns=data_without_columns_train.columns)"
      ],
      "metadata": {
        "id": "6BkgULJNmNRq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find out most important features through PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# just tried around and found out that 6 features are the best match\n",
        "pca = PCA(n_components=6).fit(data_without_columns_train)\n",
        "\n",
        "# number of components, in our case 6\n",
        "number_of_components = pca.components_.shape[0]\n",
        "\n",
        "# get the index of the most important features\n",
        "most_important = []\n",
        "for component in range(number_of_components):\n",
        "  most_important.append(abs(pca.components_[component]).argmax())\n",
        "\n",
        "initial_feature_names = data_without_columns_train.columns\n",
        "# get the names of the most important features\n",
        "most_important_names = []\n",
        "for name in range(number_of_components):\n",
        "  most_important_names.append(initial_feature_names[most_important[name]])\n",
        "\n",
        "# individual features importance in descending order\n",
        "for i in range(number_of_components):\n",
        "  print(most_important_names[i] + \" explains \" + str(np.round(pca.explained_variance_ratio_[i] * 100)) + \"% of the variance \\n\")\n",
        "\n",
        "# total explainibility of all six features combined\n",
        "print(\"\\n All six features together can explain \" + str(np.round(sum(pca.explained_variance_ratio_)*100)) + \"% of the variance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDYTT1NImTLq",
        "outputId": "923115fe-1f18-49aa-d187-39fa92523952"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L3_SO2_sensor_azimuth_angle explains 29.0% of the variance \n",
            "\n",
            "L3_NO2_sensor_zenith_angle explains 17.0% of the variance \n",
            "\n",
            "L3_NO2_solar_zenith_angle explains 12.0% of the variance \n",
            "\n",
            "weekend explains 8.0% of the variance \n",
            "\n",
            "L3_NO2_solar_azimuth_angle explains 7.0% of the variance \n",
            "\n",
            "L3_CLOUD_cloud_fraction explains 5.0% of the variance \n",
            "\n",
            "\n",
            " All six features together can explain 78.0% of the variance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reset index\n",
        "data_without_columns_train_places.index = np.arange(0, len(data_without_columns_train))\n",
        "data_without_columns_test_places.index = np.arange(0, len(data_without_columns_test))"
      ],
      "metadata": {
        "id": "XUWTLgeQmV5p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get unique count of locations\n",
        "unique_count_train = int(data_without_columns_train_places.nunique())\n",
        "unique_count_test = int(data_without_columns_test_places.nunique())\n",
        "\n",
        "# convert locations from string to category\n",
        "train_category = data_without_columns_train_places.astype({\"Place_ID\":'category'})\n",
        "test_category = data_without_columns_test_places.astype({\"Place_ID\":'category'})\n",
        "\n",
        "# get unique values of locations\n",
        "unique_values_train = train_category.Place_ID.unique()\n",
        "unique_values_test = test_category.Place_ID.unique()\n",
        "\n",
        "# encode locations\n",
        "encoding_train = np.arange(0, unique_count_train)\n",
        "encoding_test = np.arange(0, unique_count_test)\n",
        "\n",
        "encoded_locations_train = train_category.replace(\n",
        "    to_replace=unique_values_train, \n",
        "    value=encoding_train,\n",
        ")\n",
        "\n",
        "encoded_locations_test = test_category.replace(\n",
        "    to_replace=unique_values_test, \n",
        "    value=encoding_test,\n",
        ")"
      ],
      "metadata": {
        "id": "iOK9Nm4smfsP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace original locations with encodings\n",
        "data_without_columns_train[\"Place_ID\"] = encoded_locations_train\n",
        "data_without_columns_test[\"Place_ID\"] = encoded_locations_test"
      ],
      "metadata": {
        "id": "6oWyKyIBoAW3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create new columns for previous data of most important features\n",
        "# in this case we take 1 day before into account\n",
        "\n",
        "number_of_shifts = 1\n",
        "for name in most_important_names:\n",
        "    for i in range(number_of_shifts):\n",
        "        data_without_columns_train[name+'_of_'+str(i+1)+'_day_before'] = data_without_columns_train[name].shift(i+1)\n",
        "        data_without_columns_test[name+'_of_'+str(i+1)+'_day_before'] = data_without_columns_test[name].shift(i+1)"
      ],
      "metadata": {
        "id": "HxOEvYSVoMB1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reset index, so we can use it to remove NaN rows in labels\n",
        "data_without_columns_train.index = np.arange(0, len(data_without_columns_train))\n",
        "data_without_columns_test.index = np.arange(0, len(data_without_columns_test))\n",
        "data_without_columns_train_labels.index = np.arange(0, len(data_without_columns_train_labels))\n",
        "\n",
        "# through shifting we have created NaN's once again, so we need to drop these rows\n",
        "data_without_columns_train = data_without_columns_train.dropna()\n",
        "data_without_columns_test = data_without_columns_test.dropna()\n",
        "\n",
        "# also remove them from the targets\n",
        "data_without_columns_train_labels = data_without_columns_train_labels.drop(index = np.array((range(number_of_shifts))))"
      ],
      "metadata": {
        "id": "utfFz0NRoP0H"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the new and clean data\n",
        "data_without_columns_train.to_csv('LSTM_data_without_columns_train.csv')\n",
        "data_without_columns_train_labels.to_csv('LSTM_data_without_columns_train_labels.csv')\n",
        "data_without_columns_test.to_csv('LSTM_data_without_columns_test.csv')"
      ],
      "metadata": {
        "id": "ftbc6ZXroT6E"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}