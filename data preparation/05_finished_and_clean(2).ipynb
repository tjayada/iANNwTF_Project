{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_finished_and_clean(2).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "J0X4AR4dR5QI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Test data from our github repo\n",
        "url_test = 'https://raw.githubusercontent.com/tjayada/iANNwTF_Project/main/data/Test.csv'\n",
        "dataDF_test = pd.read_csv(url_test)\n",
        "\n",
        "# Load Train data from our github repo\n",
        "url_train = 'https://raw.githubusercontent.com/tjayada/iANNwTF_Project/main/data/Train.csv'\n",
        "dataDF_train = pd.read_csv(url_train)"
      ],
      "metadata": {
        "id": "j76gJVyhSClt"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data set with mean values as replacement for NaN's\n",
        "\n",
        "# for test data\n",
        "data_with_mean_NaN_test = dataDF_test.copy()\n",
        "data_with_mean_NaN_test = data_with_mean_NaN_test.fillna(data_with_mean_NaN_test.mean())\n",
        "\n",
        "# for train data\n",
        "data_with_mean_NaN_train = dataDF_train.copy()\n",
        "data_with_mean_NaN_train = data_with_mean_NaN_train.fillna(data_with_mean_NaN_train.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWL05QVnSHBw",
        "outputId": "eba96614-7b30-473b-c112-7e83971a741a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove targets and create labels\n",
        "\n",
        "data_with_mean_NaN_train_labels = data_with_mean_NaN_train[['target']]\n",
        "data_with_mean_NaN_train = data_with_mean_NaN_train.drop([\"target\", \"target_min\", \"target_max\", \"target_variance\", \"target_count\"], axis=1)"
      ],
      "metadata": {
        "id": "6OPbC1PdSpJP"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dates to datetime \n",
        "data_with_mean_NaN_test['date'] = pd.to_datetime(data_with_mean_NaN_test['Date'],format='%Y-%m-%d')\n",
        "data_with_mean_NaN_train['date'] = pd.to_datetime(data_with_mean_NaN_train['Date'],format='%Y-%m-%d')\n",
        "\n",
        "\n",
        "# add weekdays based on dates\n",
        "data_with_mean_NaN_test['weekday'] = data_with_mean_NaN_test['date'].dt.weekday\n",
        "data_with_mean_NaN_train['weekday'] = data_with_mean_NaN_train['date'].dt.weekday\n",
        "\n",
        "\n",
        "# add whether the date is a weekend or not, 0 is no and 1 is yes\n",
        "data_with_mean_NaN_test['weekend'] = np.multiply((data_with_mean_NaN_test['date'].dt.weekday >= 5), 1)\n",
        "data_with_mean_NaN_train['weekend'] = np.multiply((data_with_mean_NaN_train['date'].dt.weekday >= 5),1)\n",
        "\n",
        "\n",
        "# add month based on dates\n",
        "data_with_mean_NaN_test['month'] = data_with_mean_NaN_test['date'].dt.month\n",
        "data_with_mean_NaN_train['month'] = data_with_mean_NaN_train['date'].dt.month\n",
        "\n",
        "\n",
        "# add which day out of all days in the year it is\n",
        "data_with_mean_NaN_test['dayofyear'] = data_with_mean_NaN_test['date'].dt.dayofyear\n",
        "data_with_mean_NaN_train['dayofyear'] = data_with_mean_NaN_train['date'].dt.dayofyear\n",
        "\n",
        "\n",
        "# what quarter is the date part of\n",
        "data_with_mean_NaN_test['quarter'] = data_with_mean_NaN_test['date'].dt.quarter\n",
        "data_with_mean_NaN_train['quarter'] = data_with_mean_NaN_train['date'].dt.quarter"
      ],
      "metadata": {
        "id": "3AG95mz9SweW"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop original dates in datetime and locations\n",
        "data_with_mean_NaN_train = data_with_mean_NaN_train.drop([\"Place_ID X Date\", \"Date\", \"Place_ID\", \"date\"], axis = 1)\n",
        "data_with_mean_NaN_test = data_with_mean_NaN_test.drop([\"Place_ID X Date\", \"Date\", \"Place_ID\", \"date\"], axis = 1)"
      ],
      "metadata": {
        "id": "yfN_bobFS22t"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize data\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# for test data\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data_with_mean_NaN_test)\n",
        "scaled = scaler.fit_transform(data_with_mean_NaN_test)\n",
        "data_with_mean_NaN_test = pd.DataFrame(scaled, columns=data_with_mean_NaN_test.columns)\n",
        "\n",
        "\n",
        "# for train data\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data_with_mean_NaN_train)\n",
        "scaled = scaler.fit_transform(data_with_mean_NaN_train)\n",
        "data_with_mean_NaN_train = pd.DataFrame(scaled, columns=data_with_mean_NaN_train.columns)"
      ],
      "metadata": {
        "id": "HE28VsBpT1qP"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find out most important features through PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# just tried around and found out that 6 features are the best match\n",
        "pca = PCA(n_components=6).fit(data_with_mean_NaN_train)\n",
        "\n",
        "# number of components, in our case 6\n",
        "number_of_components = pca.components_.shape[0]\n",
        "\n",
        "# get the index of the most important features\n",
        "most_important = []\n",
        "for component in range(number_of_components):\n",
        "  most_important.append(abs(pca.components_[component]).argmax())\n",
        "\n",
        "initial_feature_names = data_with_mean_NaN_train.columns\n",
        "# get the names of the most important features\n",
        "most_important_names = []\n",
        "for name in range(number_of_components):\n",
        "  most_important_names.append(initial_feature_names[most_important[name]])\n",
        "\n",
        "# individual features importance in descending order\n",
        "for i in range(number_of_components):\n",
        "  print(most_important_names[i] + \" explains \" + str(np.round(pca.explained_variance_ratio_[i] * 100)) + \"% of the variance \\n\")\n",
        "\n",
        "# total explainibility of all six features combined\n",
        "print(\"\\n All six features together can explain \" + str(np.round(sum(pca.explained_variance_ratio_)*100)) + \"% of the variance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDCaaEW8TcYr",
        "outputId": "c30173f4-b6ba-4f5e-9464-0be81be4997a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L3_CLOUD_sensor_azimuth_angle explains 23.0% of the variance \n",
            "\n",
            "L3_NO2_sensor_zenith_angle explains 14.0% of the variance \n",
            "\n",
            "L3_O3_cloud_fraction explains 12.0% of the variance \n",
            "\n",
            "L3_CLOUD_cloud_fraction explains 8.0% of the variance \n",
            "\n",
            "weekend explains 8.0% of the variance \n",
            "\n",
            "L3_O3_solar_azimuth_angle explains 6.0% of the variance \n",
            "\n",
            "\n",
            " All six features together can explain 71.0% of the variance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create new columns for previous data of most important features\n",
        "# in this case we take 1 day before into account\n",
        "\n",
        "for name in most_important_names:\n",
        "    for i in range(1):\n",
        "        data_with_mean_NaN_train[name+'_of_'+str(i+1)+'_day_before'] = data_with_mean_NaN_train[name].shift(i+1)\n",
        "        data_with_mean_NaN_test[name+'_of_'+str(i+1)+'_day_before'] = data_with_mean_NaN_test[name].shift(i+1)"
      ],
      "metadata": {
        "id": "lcXybxjxV4rF"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# through shifting we have created NaN's once again, so we need to drop these rows\n",
        "data_without_columns_train = data_with_mean_NaN_train.dropna()\n",
        "data_without_columns_test = data_with_mean_NaN_test.dropna()"
      ],
      "metadata": {
        "id": "tE6jVHC_WU1g"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the new and clean data\n",
        "data_with_mean_NaN_train.to_csv('data_with_mean_NaN_train.csv')\n",
        "data_with_mean_NaN_train_labels.to_csv('data_with_mean_NaN_train_labels.csv')\n",
        "data_with_mean_NaN_test.to_csv('data_with_mean_NaN_test.csv')"
      ],
      "metadata": {
        "id": "P4nd76xBWgE-"
      },
      "execution_count": 57,
      "outputs": []
    }
  ]
}