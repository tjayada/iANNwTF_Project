{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \\documentclass\{article\}\
\\usepackage[utf8]\{inputenc\}\
\\usepackage\{url\}\
\\usepackage[colorlinks=true, allcolors=blue]\{hyperref\}\
\\usepackage[a4paper, total=\{150mm, 240mm\}]\{geometry\}\
\
\\title\{Tensorflow Final Project\}\
\
\\author\{\\href\{mailto:kstroemel@uos.de\}%\
\{Konstantin Str\'f6mel\}, \\href\{mailto:tdarius@uos.de\}%\
\{Tjark Darius\}, \\href\{mailto:jclaassen@uos.de\}%\
\{Johannes Claassen\}\}\
\
\\date\{March 2022\}\
\
\\usepackage\{natbib\}\
\\usepackage\{graphicx\}\
\
\\begin\{document\}\
\
\\maketitle\
\
\\section\{Introduction\}\
In 2018 the World Health Organisation discovered that 90\\% of the world's population is suffering from polluted air and that every year around 7 million people die because of air pollution. The WHO also states that it is a critical factor in many serious health problems such as lung cancer, strokes and heart diseases. Air pollution causes around one quarter of the total adult deaths through these non communicable, chronic diseases  \\citep\{who_air-pollution\}.\
And even though poor air quality makes respiratory diseases like  COVID-19 more dangerous \\citep\{iqair_empowering\}, the current pandemic also showed that during the lockdown in 2020 84\\% of the countries worldwide experienced better air quality than in the previous year \\citep\{cnn_pandemic-air-quality\}.\
So human related emissions can directly influence the air quality and our project is trying to examine these influential factors as well as implementing a model that can predict air quality based on satellite data, instead of costly and time intensive  ground-based sensors. This will potentially enable air quality monitoring and management also for low-income countries in Asia and Africa, where most of the pollution related deaths occur. In figure \\ref\{fig:monitoring\} you get an overview of the distribution of monitoring stations. Especially in South America and on the African continent there are very little ground-based measurements. Satellite data could close this big gap in the global monitoring network. \
\
\\begin\{figure\}[htb!]\
\\centering\
\\includegraphics[scale=0.52]\{PM2.5_monitoring_stations.jpg\}\
\\caption\{Overview of monitoring stations \\citep\{iqair_empowering\}\}\
\\label\{fig:monitoring\}\
\\end\{figure\}\
\
\\newpage\
\\section\{Task\}\
Our goal is to predict the PM2.5 score for different cities across the globe based on weather and satellite data \\citep\{noauthor_zindiweekendz_nodate\}.\
The PM2.5 score refers to atmospheric particulate matter with a diameter of less than 2.5 micrometers in micrograms per cubic meter air ($\\mu g / m^3$). It is one of the most widespread air pollutants, consisting of a mixture of solid and liquid particles suspended in the air. Figure \\ref\{fig:pm25_size\} shows how small PM2.5 particles are compared to a human hair or a grain of sand \\citep\{us_epa_particulate_2016\}.\
\
\\begin\{figure\}[htb!]\
\\centering\
\\includegraphics[scale=0.5]\{pm25\}\
\\caption\{PM$_\{2.5\}$ particles size comparison \\citep\{us_epa_particulate_2016\}\}\
\\label\{fig:pm25_size\}\
\\end\{figure\}\
\
\
The score ranges from 0 to 50 $\\mu g / m^3$ per 24 hours. The table in figure \\ref\{fig:pm25\} shows the WHO guidelines for PM2.5 pollution. It also serves as a color code for the global PM2.5 map in figure \\ref\{fig:pm25_map\} \\citep\{iqair_empowering\}. \
\
\\begin\{figure\}[htb!]\
\\centering\
\\includegraphics[scale=0.8]\{WHO_guidelines.jpg\}\
\\caption\{WHO PM2.5 guideline \\citep\{iqair_empowering\}\}\
\\label\{fig:pm25\}\
\\end\{figure\}\
\
\\begin\{figure\}[htb!]\
\\centering\
\\includegraphics[scale=0.8]\{global_PM2.5_map.jpg\}\
\\caption\{Global PM2.5 map \\citep\{iqair_empowering\}\}\
\\label\{fig:pm25_map\}\
\\end\{figure\}\
For this task we had to solve a so-called "out of distribution" problem, because we had to predict target values for cities that were not part of the training data. \
In the following section we will describe the available training data and how we used it for our model. \
\\newpage\
\\section\{Data\}\
The data provided by \\textit\{Zindi\} comprises of three different main sources of data. First, the target values, which are five columns in total, are obtained from ground-based air quality sensors. Second, there are six columns of general weather data produced by the \\href\{https://nomads.ncep.noaa.gov/txt_descriptions/GFS_doc.shtml\}\{\
Global Forecast System\}, operated by the U.S. National Weather Service. And third, there are in sum 68 columns of satellite data, provided by the \\href\{https://sentinel.esa.int/web/sentinel/missions/sentinel-5p\}\{Sentinel-5 Precursor\} project.\
\
\\subsection\{Target data\}\
The column \{\\fontfamily\{qcr\}\\selectfont target\} displays the PM$_\{2.5\}$ particle concentration, which describes the concentration of fine particles of diameter less than 2.5 $\\mu$m. These are of great public interest due to their health impact  \\citep\{tai_meteorological_2012\}.\
The remaining four target columns (namely \{\\fontfamily\{qcr\}\\selectfont target\\_min\}, \{\\fontfamily\{qcr\}\\selectfont target\\_max\}, \{\\fontfamily\{qcr\}\\selectfont target\\_variance\} and \{\\fontfamily\{qcr\}\\selectfont target\\_count\}) are summary statistics of the target variable for each entry.\
\
\\subsection\{Weather data\}\
As mentioned, the weather data is obtained from the \\textit\{Global Forecast System\}. This is no measured data but predicted data produced by the FV3 model developed by the U.S. National Weather Service. Since we use data from January 2020 to April 2020, the model ran in version 15.2  \\citep\{emc_gfs_17-03-2022\}. The model output consists of nine variables of which six are used in the data set, \{\\fontfamily\{qcr\}\\selectfont precipitable\\_water\\_entire\\_atmosphere\} describes the amount of water that is precipitable for the whole atmosphere in $kg/m^2$, \\\\ \{\\fontfamily\{qcr\}\\selectfont relative\\_humidity\\_2m\\_above\\_ground\} gives the relative air humidity in percent at two meters height,  \{\\fontfamily\{qcr\}\\selectfont specific\\_humidity\\_2m\\_above\\_ground\} refers to the mass of water in a specific amount air in $kg/kg$,\\\\ \{\\fontfamily\{qcr\}\\selectfont temperature\\_2m\\_above\\_ground\} gives the air temperature at two meters height, \{\\fontfamily\{qcr\}\\selectfont u\\_component\\_of\\_wind\\_10m\\_above\\_ground\} describes the $u$-component of the wind, i.e., the vector showing the wind direction between east and west and \{\\fontfamily\{qcr\}\\selectfont v\\_component\\_of\\_wind\\_10m\\_above\\_ground\} giving the $v$-component, i.e., the wind direction in terms of north or south \\citep\{google_gfs_17-03-2022\}.\
\
\\subsection\{Satellite data\}\
The satellite data is gathered by the Copernicus Sentinel-5 Precursor mission, conducted by a collaboration of the European Space Agency (ESA), the European Commision and the Netherlands Space Office. The data is collected by the Tropospheric Monitoring Instrument (TROPOMI) carried by the Sentinel-5P satellite which was launched in October 2017. Its main goal is to perform atmospheric measurements in order to monitor parameters such as air quality, UV radiation or climate change  \\citep\{esa_sentinel-mission-page_nodate\}.\
It measures the tropospheric concentration of substances associated with air quality, such as carbon monoxide (CO), formaldehyde (HCHO), ozone (O$_3$), sulphur dioxide (SO$_2$), nitrogen dioxide (NO$_2$) and those associated with climate forcing, for instance methane (CH$_4$) and water vapour (H$_2$O) \\citep\{esa_thematic_nodate\}. \\\\\
As mentioned, the satellite data consists of 68 columns covering the substances NO$_2$, O$_3$, CO, HCHO, H$_2$O (i.e., clouds), SO$_2$, CH$_4$ and produces the \\href\{http://www.tropomi.eu/data-products/uv-aerosol-index\}\{UV Aerosol Index\}. \\\\\
For instance, the data set contains twelve columns for NO$_2$ (starting with \{\\fontfamily\{qcr\}\\selectfont L3\\_NO2\\_\}). Four of these variables are actual measurements. \{\\fontfamily\{qcr\}\\selectfont column\\_number\\_density\} displays the vertical column density\\footnote\{For more information, refer to https://sentinels.copernicus.eu/web/sentinel/technical-guides/sentinel-5p/level-2/doas-method\} of NO$_2$ divided by the total air mass factor for the whole atmosphere. \{\\fontfamily\{qcr\}\\selectfont tropospheric\\_NO2\\_column\\_number\\_density\} and \{\\fontfamily\{qcr\}\\selectfont stratospheric\\_NO2\\_column\\_number\\_density\} show the same just for the troposphere and stratosphere, respectively. \
\
\\citep\{noauthor_sentinel-5p_nodate\}\
\\subsection\{Data preprocessing\}\
The provided training data described above has 30557 entries in total, spanning over 82 features. Only 12 of these features have no missing values while the other features range between missing a couple hundred values to over 24000 missing values. \\\\\
Simply removing each entry containing a missing value would result in a data set with only 3915 entries left. So to solve this problem of missing values we came up with two possible solutions:\
\
\\subsubsection\{Remove features with most missing values\}\
The first approach is based on the fact that most features contain more than 23000 complete entries and only a couple features have more than 80 \\% of their data missing. So the idea is to exclude these mainly incomplete features and afterwards remove the entries with missing data. This results in a data set where the columns detailing methane (CH$_4$) concentration and respective satellite meta data are removed entirely. After removing individual entries with missing data, the data set then has 18219 entries in total spanning over 75 features.\
\
\\subsubsection\{Fill in all missing values\}\
The second approach is trying to use all the data we are provided with as best as possible. So we calculated the mean of each feature and then replaced missing values with the respective mean value. This method is somewhat questionable, since some features consist of mainly missing data, but we were interested how our models would perform on each of these two data sets.\\\\\
The results were that all models performed marginally better on the data set with the removed methane columns than on the data set with filled in mean-values.\
\
\\subsubsection\{Adding and removing features\}\
Independent of the approach we used for the missing values, we also added some features relating to dates and times. Since the dates were provided in the data set, we then used this information to add a feature that tells us what weekday the date is, whether the given date is on a weekend or not, which month and season it belongs to and what day of the year the provided date is. With these new features we then could remove the features that we could not use based on the object types, which were the Place\\_ID's and Dates. \\\\ \
After normalizing the data set using the MinMaxScaler from the sklearn library \
\\citep\{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\} \
we used Principal Component Analysis (PCA)  \
\\citep\{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\} \
to determine which features are considered important for the variance of the data set and thus useful to add previous time steps to the data set. In total six features were found to be responsible for more than 75\\% of the data sets variance. These six features were then added to the data set with a shift of one, such that each data point would have the information for these six features of the previous day as well. It would have been possible to add the data of more than one day, for example a whole week previous to the given data points date, but since the data set has many different locations, these shifts would end up adding data of previous days to locations that are possibly independent of each other. So to not mix up different locations with each other, while still getting the benefit of these useful previous features, we decided to only shift by one day.\
\
\\section\{Related work\}\
In this section we will review previous studies on air quality prediction by researchers, who employed different deep learning techniques. \
Ong, Sugiura and Zettsu proposed a deep recurrent neural network (DRNN) specifically designed for PM2.5 prediction. Their network is improved by a novel dynamical pre-training method. They use stacked autoencoders to build up their RNN. In this concatenation of autoencoders the output of the model of the layer below serves as input for the next autoencoder. Due to that each hidden layer is a higher-level abstraction of the previous layer, therefore the last hidden layer contains the high-level structure and representative information of the input.\
This results in the big advantage that the network can select relevant sensors for its predictions. They achieve this through a regularized regression technique called elastic net (EN). The training data is often quite sparse and incomplete. Through the EN they were able to filter out sensors, that did not improve the predictions significantly. This reduces the overall computational costs and results in a more interpretable response\'96predictor relationship \\citep\{ong_dynamically_2016\}.\
\
In 2019 Wen et al. developed a novel neural network arcitecture for air pollution prediction. It is a combination of a convolutional neural network (CNN) and a long short-term memory (LSTM) network. In figure \\ref\{fig:c_lstm\} their C-LSTM model is sketched out. They argue that this combination is beneficial due to the combination of temporal and spatial features in the training data\\citep\{wen_novel_2019\}.\
\
\\begin\{figure\}[htb!]\
\\centering\
\\includegraphics[scale=1.25]\{C_LSTM model.jpg\}\
\\caption\{C-LSTM model \\citep\{wen_novel_2019\}\}\
\\label\{fig:c_lstm\}\
\\end\{figure\}\
Chae et al. used an interpolated convolutional neural network (ICNN) for their predictions of air pollution in South Korea. CNNs work best with an evenly spaced grid like data. To achieve this spatially balanced structure they made use of interpolation. The pollution monitoring stations are concentrated in bigger cities with unequal distances between the stations. They created an equally spaced empty grid and filled in data points as if a virtual measuring station was located at every grid point through interpolation from the existing measurements \\citep\{chae_pm10_2021\}. Figure \\ref\{fig:icnn\} visualizes their network architecture.\
\
\\begin\{figure\}[htb!]\
\\centering\
\\includegraphics[scale=0.22]\{ICNN.jpg\}\
\\caption\{ICNN model \\citep\{chae_pm10_2021\}\}\
\\label\{fig:icnn\}\
\\end\{figure\}\
\\newpage\
\\section\{Model\}\
Due to the chronologically ordered data we believe a LSTM is the best choice for predicting the desired targets. Since these Long Short-Term Memory models are also being actively used for time series forecasting like the weather or stock markets. This special kind of Recurrent Neural Network would be a good choice, if our data did not include over 300 different locations and the target locations we want to predict were part of the training data, but the locations for training and predicting are entirely different from each other. This problem led us to try multiple models that could disregard the chronological order and different locations by only using the weather and satellite data itself. As mentioned in the Data preprocessing section, we have included the data of the six most important features of the previous data points date and thus managed to include this information without the data being necessarily chronologically ordered. \\\\\
We always use the same hyperparameter, error function and optimizer to be able to compare the different model structures better. We use a training loop with 100 epochs, the Mean Squared Error function and the Adam optimizer with a learning rate of 0.01 for all models.\
The four different models we used are:\
\\subsection\{Linear Regression\}\
The first and  most simple idea is to use a linear regression model to predict the target values. To our surprise the linear regression model performed rather well with a Root Mean Squared Error (RMSE) of 37.6 for both data sets. \\\\\
As the name may suggest this model only has one layer and one unit with a linear activation function, but this simple structure was only marginally worse than the next model we used, which was a Multi-Layer Perceptron (MLP).\
\\subsection\{MLP\}\
The MLP is a better fit for our problem, since the complex data is most likely not best described through a single linear function, but rather multiple (non-)linear functions. So the input layer has as many units as our data features, which is either 78 for the first data set without the methane columns or 85 features for the data set with the mean filled missing values. These input layer units have a linear activation function, while the next three hidden layers use a rectified linear activation function. These hidden layers double each time in unit size, so the first on has 160, then 320 and then 160 again. The output layer has again only one unit, since we only want to predict one value, with a linear activation function. We got a RMSE of 30.6 for the first data set and a RMSE of 33.3 for the second one. As mentioned in the data preprocessing section, all models performed slightly better on the first data set. The next model we try is a Convolutional Neural Network (CNN).\
\
\\subsection\{CNN\}\
The CNN is the first unconventional model for this kind of task. To be able to use the CNN we need to add an extra empty dimension to our data, so the shape is then \
\{\\fontfamily\{qcr\}\\selectfont (number of entries, number of features, 1) \}. The CNN itself consists of five 1-dimensional convolutional layers (also called temporal convolution) starting with 32 filters and then doubling for each layer up to 128 filters in the third layer and then halving again back to 32 filters in the fifth layer. The kernel size 2 and the ReLU activation function stay the same across each layer. Then we flatten the input and use simple dense layers for the output. We are able to get a RMSE of 31.9 for the first data set and a RMSE of 33.8 for the second data set.\
\
\\subsection\{LSTM\}\
Lastly we implement the LSTM which we deemed the most promising model for this problem and data. Due to time constraints we were not able to implement the LSTM the same as the other models, but rather we were using the functional API of TensorFlow, but we do not consider this a problem in the comparison to the other models. The model achieved a RMSE of 30.6, equal to the MLP, for the first data set and the best so far for the second data set with a RMSE of 31.4 with the same parameters as the other models. \\\\\
But as we think that this is the best approach to our problem, we then tried to get it perform the best we could. We implemented a model with a LSTM with 200 units and to counter the overfitting a l2 regularizer of 0.001. Afterwards we used a dropout layer with a value of 0.5, which means that half the neural connections would be dropped. Also in response to the overfitting was an early-stopping function used, and at the end a linear dense output layer with 1 unit.\
We trained for 200 epochs and were using the Nadam optimizer with a learning rate of 0.01.\
To be able to use the \
\{\\fontfamily\{qcr\}\\selectfont PlaceID \} \
we encoded the different locations, but to our surprise the LSTM performed worse with unshuffled data that also has locations, even though the LSTM should in theory profit from chronologically ordered data that can be distinguished by different locations. \\\\\
Below are the 4 different LSTM configurations we have implemented. On the top left etc ..\
\
% insert Picture with 4 different graphs of performances here\
\\begin\{figure\}[htb!]\
\\centering\
\\includegraphics[scale=0.3]\{model_comparison2.jpg\}\
\\caption\{Model comparison\}\
\\label\{fig:model_comparison\}\
\\end\{figure\}\
\\newpage\
\\section\{Results\}\
Throughout the paper we already mentioned multiple interesting results. For once we determined that the first data set with the removed methane columns was better suited for training and predictions than the second data set that used the mean to fill in missing values. This can most likely be explained by the fact that filling in too many missing values actually changes the data in a way that does not reflect the original data distribution anymore. Another observation was that the MLP and LSTM without locations were the best performing models. The linear regression model assumes a way too simple data distribution and the CNN model structure is not necessarily build for this regression task with (originally) chronological structured data. Another interesting point is that the LSTM actually performed worse when we added the location data and did not shuffle the data to keep the chronological structure of the data. \\\\\
Regarding the data set we found out that certain features are more important in explaining the distribution than others. For example the feature \
\{\\fontfamily\{qcr\}\\selectfont L3-SO2-sensor-azimuth-angle \}\
explains 29\\% of the variance of the data set or the fourth most important feature \
\{\\fontfamily\{qcr\}\\selectfont weekend \}\
was added artificially by us and explains 8 \\% of the data sets variance. And more than $\\frac\{3\}\{4\}$ of the data sets variance can be explained by only 6 of the around 80 different features.\
\
\\section\{Outlook\}\
Due to the limited realm of our project we were not able to employ hyperparameter optimization and test further regularization techniques. In the future another interesting approach would be to use the location (Place-ID) with an improved encoding, so the LSTM and maybe the other models as well, could profit from this, instead of worsening the performance. We consciously decide against this, since we were dealing with an out of distribution problem, in the sense that the location id would have been useless for the prediction, since they were entirely different from each other, but maybe a model might profit from the location, even though it did not work for our simple encoding scheme.\
- also: avoid overfitting in CNN\
All in all we are satisfied with our work but improvement possible\
\\section\{Conclusion\}\
We did not outperform the winning model of the Zindi challenge, but are confident to be at least in the top ten ;) \
Satisfied etc.\
Important topic (predicting pm2.5) proud to be part of that etc. \
Give attention to this topic, broaden our minds and hopefully yours as wells.\
like to see limitation/outlook section be improved in the future.\
\\bibliographystyle\{ieeetr\}\
\\bibliography\{references\}\
\
\
\\end\{document\}\
\
###########################################\
\
N\'e4chstes File:\
Exportierte Eintr\'e4ge:\
\
\
@article\{li_long_2017,\
	title = \{Long short-term memory neural network for air pollutant concentration predictions: \{Method\} development and evaluation\},\
	volume = \{231\},\
	issn = \{1873-6424\},\
	shorttitle = \{Long short-term memory neural network for air pollutant concentration predictions\},\
	doi = \{10.1016/j.envpol.2017.08.114\},\
	abstract = \{Air pollutant concentration forecasting is an effective method of protecting public health by providing an early warning against harmful air pollutants. However, existing methods of air pollutant concentration prediction fail to effectively model long-term dependencies, and most neglect spatial correlations. In this paper, a novel long short-term memory neural network extended (LSTME) model that inherently considers spatiotemporal correlations is proposed for air pollutant concentration prediction. Long short-term memory (LSTM) layers were used to automatically extract inherent useful features from historical air pollutant data, and auxiliary data, including meteorological data and time stamp data, were merged into the proposed model to enhance the performance. Hourly PM2.5 (particulate matter with an aerodynamic diameter less than or equal to 2.5\'a0\uc0\u956 m) concentration data collected at 12 air quality monitoring stations in Beijing City from Jan/01/2014 to May/28/2016 were used to validate the effectiveness of the proposed LSTME model. Experiments were performed using the spatiotemporal deep learning (STDL) model, the time delay neural network (TDNN) model, the autoregressive moving average (ARMA) model, the support vector regression (SVR) model, and the traditional LSTM NN model, and a comparison of the results demonstrated that the LSTME model is superior to the other statistics-based models. Additionally, the use of auxiliary data improved model performance. For the one-hour prediction tasks, the proposed model performed well and exhibited a mean absolute percentage error (MAPE) of 11.93\\%. In addition, we conducted multiscale predictions over different time spans and achieved satisfactory performance, even for 13-24\'a0h prediction tasks (MAPE\'a0=\'a031.47\\%).\},\
	language = \{eng\},\
	number = \{Pt 1\},\
	journal = \{Environmental Pollution (Barking, Essex: 1987)\},\
	author = \{Li, Xiang and Peng, Ling and Yao, Xiaojing and Cui, Shaolong and Hu, Yuan and You, Chengzeng and Chi, Tianhe\},\
	month = dec,\
	year = \{2017\},\
	pmid = \{28898956\},\
	keywords = \{Air pollutant concentration predictions, Air Pollutants, Air Pollution, Beijing, Cities, Environmental Monitoring, Forecasting, Long short-term memory neural network (LSTM NN), Models, Statistical, Models, Theoretical, Multiscale prediction, Neural Networks, Computer, Particulate Matter, Recurrent neural network, Spatiotemporal correlation\},\
	pages = \{997--1004\},\
\}\
\
##############################################################\
\
N\'e4chstes File:\
references;\
\
\
@misc\{noauthor_zindiweekendz_nodate,\
	title = \{\\#\{ZindiWeekendz\} \{Learning\}: \{Urban\} \{Air\} \{Pollution\} \{Challenge\}\},\
	shorttitle = \{\\#\{ZindiWeekendz\} \{Learning\}\},\
	url = \{https://zindi.africa/competitions/zindiweekendz-learning-urban-air-pollution-challenge/data\},\
	howpublished = \{https://zindi.africa/competitions/zindiweekendz-learning-urban-air-pollution-challenge/data\},\
	note = \{accessed 26.03.2022\},\
	abstract = \{Can you predict air quality in cities around the world using satellite data?\},\
	language = \{en\},\
	urldate = \{2022-03-10\},\
	journal = \{Zindi\},\
\}\
\
@misc\{google_gfs_17-03-2022,\
	title = \{\{GFS\}: \{Global\} \{Forecast\} \{System\} 384-\{Hour\} \{Predicted\} \{Atmosphere\} \{Data\} \{\\textbar\} \{Earth\} \{Engine\} \{Data\} \{Catalog\}\},\
	shorttitle = \{\{GFS\}\},\
	howpublished = \{https://developers.google.com/earth-engine/datasets/catalog/NOAA\\_GFS0P25\},\
	url = \{https://developers.google.com/earth-engine/datasets/catalog/NOAA_GFS0P25\},\
	abstract = \{The Global Forecast System (GFS) is a weather forecast model produced by the National Centers for Environmental Prediction (NCEP). The GFS dataset consists of selected model outputs (described below) as gridded forecast variables. The 384-hour forecasts, with 3-hour forecast interval, are made at 6-hour temporal resolution (i.e. updated four times \'85\},\
	language = \{en\},\
	urldate = \{2022-03-10\},\
	note = \{accessed 17.03.2022\},\
	journal = \{Google Developers\},\
\}\
\
@misc\{noauthor_sentinel-5p_nodate,\
	title = \{Sentinel-\{5P\} \{OFFL\} \{NO2\}: \{Offline\} \{Nitrogen\} \{Dioxide\} \{\\textbar\} \{Earth\} \{Engine\} \{Data\} \{Catalog\} \{\\textbar\} \{Google\} \{Developers\}\},\
	shorttitle = \{Sentinel-\{5P\} \{OFFL\} \{NO2\}\},\
	url = \{https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_NO2\},\
	abstract = \{OFFL/L3\\_NO2 This dataset provides offline high-resolution imagery of NO2 concentrations. Nitrogen oxides (NO2 and NO) are important trace gases in the Earth\'92s atmosphere, present in both the troposphere and the stratosphere. They enter the atmosphere as a result of anthropogenic activities (notably fossil fuel combustion and biomass burning) and natural \'85\},\
	language = \{en\},\
	howpublished = \{https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS\\_S5P\\_OFFL\\_L3\\_NO2\},\
	note = \{accessed 27.03.2022\},\
	urldate = \{2022-03-10\},\
\}\
\
@misc\{noauthor_sentinel-5p_nodate-1,\
	title = \{Sentinel-\{5P\} \{OFFL\} \{O3\}: \{Offline\} \{Ozone\} \{\\textbar\} \{Earth\} \{Engine\} \{Data\} \{Catalog\} \{\\textbar\} \{Google\} \{Developers\}\},\
	shorttitle = \{Sentinel-\{5P\} \{OFFL\} \{O3\}\},\
	url = \{https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_O3\},\
	abstract = \{OFFL/L3\\_O3 This dataset provides offline high-resolution imagery of total column ozone concentrations. See also COPERNICUS/S5P/OFFL/L3\\_O3\\_TCL for the tropospheric column data. In the stratosphere, the ozone layer shields the biosphere from dangerous solar ultraviolet radiation. In the troposphere, it acts as an efficient cleansing agent, but at high concentration it also \'85\},\
	language = \{en\},\
	urldate = \{2022-03-10\},\
\}\
\
@misc\{noauthor_sentinel-5p_nodate-2,\
	title = \{Sentinel-\{5P\} \{OFFL\} \{HCHO\}: \{Offline\} \{Formaldehyde\} \{\\textbar\} \{Earth\} \{Engine\} \{Data\} \{Catalog\} \{\\textbar\} \{Google\} \{Developers\}\},\
	shorttitle = \{Sentinel-\{5P\} \{OFFL\} \{HCHO\}\},\
	url = \{https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_HCHO\},\
	abstract = \{OFFL/L3\\_HCHO This dataset provides offline high-resolution imagery of atmospheric formaldehyde (HCHO) concentrations. Formaldehyde is an intermediate gas in almost all oxidation chains of non-methane volatile organic compounds (NMVOC), leading eventually to CO2. Non-Methane Volatile Organic Compounds (NMVOCs) are, together with NOx, CO and CH4, among the most important precursors of \'85\},\
	language = \{en\},\
	urldate = \{2022-03-10\},\
\}\
\
@misc\{noauthor_sentinel-5p_nodate-3,\
	title = \{Sentinel-\{5P\} \{OFFL\} \{CLOUD\}: \{Near\} \{Real\}-\{Time\} \{Cloud\} \{\\textbar\} \{Earth\} \{Engine\} \{Data\} \{Catalog\} \{\\textbar\} \{Google\} \{Developers\}\},\
	shorttitle = \{Sentinel-\{5P\} \{OFFL\} \{CLOUD\}\},\
	url = \{https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CLOUD\},\
	abstract = \{OFFL/L3\\_CLOUD This dataset provides offline high-resolution imagery of cloud parameters. The TROPOMI/S5P cloud properties retrieval is based on the OCRA and ROCINN algorithms currently being used in the operational GOME and GOME-2 products. OCRA retrieves the cloud fraction using measurements in the UV/VIS spectral regions and ROCINN retrieves the cloud \'85\},\
	language = \{en\},\
	urldate = \{2022-03-10\},\
\}\
\
@misc\{noauthor_sentinel-5p_nodate-4,\
	title = \{Sentinel-\{5P\} \{OFFL\} \{AER\} \{AI\}: \{Offline\} \{UV\} \{Aerosol\} \{Index\} \{\\textbar\} \{Earth\} \{Engine\} \{Data\} \{Catalog\} \{\\textbar\} \{Google\} \{Developers\}\},\
	shorttitle = \{Sentinel-\{5P\} \{OFFL\} \{AER\} \{AI\}\},\
	url = \{https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_AER_AI\},\
	abstract = \{OFFL/L3\\_AER\\_AI This dataset provides offline high-resolution imagery of the UV Aerosol Index (UVAI), also called the Absorbing Aerosol Index (AAI). The AAI is based on wavelength-dependent changes in Rayleigh scattering in the UV spectral range for a pair of wavelengths. The difference between observed and modelled reflectance results in the \'85\},\
	language = \{en\},\
	urldate = \{2022-03-10\},\
\}\
\
@misc\{noauthor_sentinel-5p_nodate-5,\
	title = \{Sentinel-\{5P\} \{OFFL\} \{SO2\}: \{Offline\} \{Sulphur\} \{Dioxide\} \{\\textbar\} \{Earth\} \{Engine\} \{Data\} \{Catalog\} \{\\textbar\} \{Google\} \{Developers\}\},\
	shorttitle = \{Sentinel-\{5P\} \{OFFL\} \{SO2\}\},\
	url = \{https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_SO2\},\
	abstract = \{OFFL/L3\\_SO2 This dataset provides offline high-resolution imagery of atmospheric sulfur dioxide (SO2) concentrations. Sulphur dioxide (SO2) enters the Earth\'92s atmosphere through both natural and anthropogenic processes. It plays a role in chemistry on a local and global scale and its impact ranges from short-term pollution to effects on climate. Only \'85\},\
	language = \{en\},\
	urldate = \{2022-03-10\},\
\}\
\
@misc\{noauthor_sentinel-5p_nodate-6,\
	title = \{Sentinel-\{5P\} \{OFFL\} \{CH4\}: \{Offline\} \{Methane\} \{\\textbar\} \{Earth\} \{Engine\} \{Data\} \{Catalog\} \{\\textbar\} \{Google\} \{Developers\}\},\
	shorttitle = \{Sentinel-\{5P\} \{OFFL\} \{CH4\}\},\
	url = \{https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CH4\},\
	abstract = \{OFFL/L3\\_CH4 This dataset provides offline high-resolution imagery of methane concentrations. Methane (CH4) is, after carbon dioxide (CO2), the most important contributor to the anthropogenically enhanced greenhouse effect. Roughly three-quarters of methane emissions are anthropogenic and as such it is important to continue the record of satellite based measurements. TROPOMI aims \'85\},\
	language = \{en\},\
	urldate = \{2022-03-10\},\
\}\
\
@misc\{noauthor_global_nodate,\
	title = \{Global \{Forecast\} \{System\}\},\
	url = \{https://nomads.ncep.noaa.gov/txt_descriptions/GFS_doc.shtml\},\
	urldate = \{2022-03-16\},\
\}\
\
@article\{tai_meteorological_2012,\
	title = \{Meteorological modes of variability for fine particulate matter (\{PM\}$_\{2.5\}$) air quality in the  \{United\} \{States\}: implications for \{PM\}$_\{2.5\}$ sensitivity to climate change\},\
	volume = \{12\},\
	issn = \{1680-7324\},\
	shorttitle = \{Meteorological modes of variability for fine particulate matter (\{PM\}\\&lt;sub\\&gt;2.5\\&lt;/sub\\&gt;) air quality in the \{United\} \{States\}\},\
	url = \{https://acp.copernicus.org/articles/12/3131/2012/\},\
	doi = \{10.5194/acp-12-3131-2012\},\
	abstract = \{Abstract. We applied a multiple linear regression model to understand the relationships of PM2.5 with meteorological variables in the contiguous US and from there to infer the sensitivity of PM2.5 to climate change. We used 2004\'962008 PM2.5 observations from \{\\textasciitilde\}1000 sites (\{\\textasciitilde\}200 sites for PM2.5 components) and compared to results from the GEOS-Chem chemical transport model (CTM). All data were deseasonalized to focus on synoptic-scale correlations. We find strong positive correlations of PM2.5 components with temperature in most of the US, except for nitrate in the Southeast where the correlation is negative. Relative humidity (RH) is generally positively correlated with sulfate and nitrate but negatively correlated with organic carbon. GEOS-Chem results indicate that most of the correlations of PM2.5 with temperature and RH do not arise from direct dependence but from covariation with synoptic transport. We applied principal component analysis and regression to identify the dominant meteorological modes controlling PM2.5 variability, and show that 20\'9640\\% of the observed PM2.5 day-to-day variability can be explained by a single dominant meteorological mode: cold frontal passages in the eastern US and maritime inflow in the West. These and other synoptic transport modes drive most of the overall correlations of PM2.5 with temperature and RH except in the Southeast. We show that interannual variability of PM2.5 in the US Midwest is strongly correlated with cyclone frequency as diagnosed from a spectral-autoregressive analysis of the dominant meteorological mode. An ensemble of five realizations of 1996\'962050 climate change with the GISS general circulation model (GCM) using the same climate forcings shows inconsistent trends in cyclone frequency over the Midwest (including in sign), with a likely decrease in cyclone frequency implying an increase in PM2.5. Our results demonstrate the need for multiple GCM realizations (because of climate chaos) when diagnosing the effect of climate change on PM2.5, and suggest that analysis of meteorological modes of variability provides a computationally more affordable approach for this purpose than coupled GCM-CTM studies.\},\
	language = \{en\},\
	number = \{6\},\
	urldate = \{2022-03-17\},\
	journal = \{Atmospheric Chemistry and Physics\},\
	author = \{Tai, A. P. K. and Mickley, L. J. and Jacob, D. J. and Leibensperger, E. M. and Zhang, L. and Fisher, J. A. and Pye, H. O. T.\},\
	month = mar,\
	year = \{2012\},\
	pages = \{3131--3145\},\
\}\
@article\{al-janabi_new_2020,\
	title = \{A new method for prediction of air pollution based on intelligent computation\},\
	volume = \{24\},\
	issn = \{1433-7479\},\
	url = \{https://doi.org/10.1007/s00500-019-04495-1\},\
	doi = \{10.1007/s00500-019-04495-1\},\
	abstract = \{The detection and treatment of increasing air pollution due to technological developments represent some of the most important challenges facing the world today. Indeed, there has been a significant increase in levels of environmental pollution in recent years. The aim of the work presented herein is to design an intelligent predictor for the concentrations of air pollutants over the next 2\'a0days based on deep learning techniques using a recurrent neural network (RNN). The best structure for its operation is then determined using a particle swarm optimization (PSO) algorithm. The new predictor based on intelligent computation relying on unsupervised learning, i.e., long short-term memory (LSTM) and optimization (i.e., PSO), is called the smart air quality prediction model (SAQPM). The main goal is to predict six the concentrations of six types of air pollution, viz. PM2.5 particulate matter, PM10, particulate matter, nitrogen dioxide (NO2), carbon monoxide (CO), ozone (O3), and sulfur dioxide (SO2). SAQPM consists of four stages. The first stage involves data collection from multiple stations (35 in this case). The second stage involves preprocessing of the data, including (a) separation of each station with an independent focus, (b) handle missing values, and (c) normalization of the dataset to the range of (0, 1) using the MinMaxScalar method. The third stage relates to building the predictor based on the LSTM method by identifying the best structure and parameter values (weight, bias, number of hidden layers, number of nodes in each hidden layer, and activation function) for the network using the functional PSO algorithm to achieve a goal. Thereafter, the dataset is split into training and testing parts based on the ten cross-validation principle. The training dataset is then used to build the predictor. In the fourth stage, evaluation results for each station are obtained by reading the concentration of each pollutant each hour for at most 30\'a0days then taking the average of the symmetric mean absolute percentage error (SMAPE) for 25\'a0days only.\},\
	language = \{en\},\
	number = \{1\},\
	urldate = \{2022-03-28\},\
	journal = \{Soft Computing\},\
	author = \{Al-Janabi, Samaher and Mohammad, Mustafa and Al-Sultan, Ali\},\
	month = jan,\
	year = \{2020\},\
	pages = \{661--680\},\
	file = \{Springer Full Text PDF:C\\:\\\\Users\\\\konst\\\\Zotero\\\\storage\\\\446L49Z5\\\\Al-Janabi et al. - 2020 - A new method for prediction of air pollution based.pdf:application/pdf\},\
\}\
\
@article\{ong_dynamically_2016,\
	title = \{Dynamically pre-trained deep recurrent neural networks using environmental monitoring data for predicting \{PM2\}.5\},\
	volume = \{27\},\
	issn = \{1433-3058\},\
	url = \{https://doi.org/10.1007/s00521-015-1955-3\},\
	doi = \{10.1007/s00521-015-1955-3\},\
	abstract = \{Fine particulate matter (\\$\\$\{\\textbackslash\}hbox \\\{PM\\\}\\_\\\{2.5\\\}\\$\\$) has a considerable impact on human health, the environment and climate change. It is estimated that with better predictions, US\\$9 billion can be saved over a 10-year period in the USA (State of the science fact sheet air quality. http://www.noaa.gov/factsheets/new, 2012). Therefore, it is crucial to keep developing models and systems that can accurately predict the concentration of major air pollutants. In this paper, our target is to predict \\$\\$\{\\textbackslash\}hbox \\\{PM\\\}\\_\\\{2.5\\\}\\$\\$concentration in Japan using environmental monitoring data obtained from physical sensors with improved accuracy over the currently employed prediction models. To do so, we propose a deep recurrent neural network (DRNN) that is enhanced with a novel pre-training method using auto-encoder especially designed for time series prediction. Additionally, sensors selection is performed within DRNN without harming the accuracy of the predictions by taking advantage of the sparsity found in the network. The numerical experiments show that DRNN with our proposed pre-training method is superior than when using a canonical and a state-of-the-art auto-encoder training method when applied to time series prediction. The experiments confirm that when compared against the \\$\\$\{\\textbackslash\}hbox \\\{PM\\\}\\_\\\{2.5\\\}\\$\\$prediction system VENUS (National Institute for Environmental Studies. Visual Atmospheric Environment Utility System. http://envgis5.nies.go.jp/osenyosoku/, 2014), our technique improves the accuracy of \\$\\$\{\\textbackslash\}hbox \\\{PM\\\}\\_\\\{2.5\\\}\\$\\$concentration level predictions that are being reported in Japan.\},\
	language = \{en\},\
	number = \{6\},\
	urldate = \{2022-03-28\},\
	journal = \{Neural Computing and Applications\},\
	author = \{Ong, Bun Theang and Sugiura, Komei and Zettsu, Koji\},\
	month = aug,\
	year = \{2016\},\
	pages = \{1553--1566\},\
	file = \{Springer Full Text PDF:C\\:\\\\Users\\\\konst\\\\Zotero\\\\storage\\\\637S389J\\\\Ong et al. - 2016 - Dynamically pre-trained deep recurrent neural netw.pdf:application/pdf\},\
\}\
\
@article\{wen_novel_2019,\
	title = \{A novel spatiotemporal convolutional long short-term neural network for air pollution prediction\},\
	volume = \{654\},\
	issn = \{0048-9697\},\
	url = \{https://www.sciencedirect.com/science/article/pii/S0048969718344413\},\
	doi = \{10.1016/j.scitotenv.2018.11.086\},\
	abstract = \{Air pollution is a serious environmental problem that has drawn worldwide attention. Predicting air pollution in advance has great significance on people's daily health control and government decision-making. However, existing research methods have failed to effectively extract the spatiotemporal features of air pollutant concentration data, and exhibited low precision in long-term predictions and sudden changes in air quality. In the present study, a spatiotemporal convolutional long short-term memory neural network extended (C-LSTME) model for predicting air quality concentration was proposed. In order to encompass the spatiality and temporality of the data, the model involved the historical air pollutant concentration of the present station, as well as that of the adaptive k-nearest neighboring stations, into the model. High-level spatiotemporal features were extracted through the combination of the convolutional neural network (CNN) and long short-term memory neural network (LSTM-NN), and meteorological data and aerosol data were also integrated, in order to improve model prediction performance. Hourly PM2.5 (particulate matter with an aerodynamic diameter of \uc0\u8804 2.5\u8239 mm) concentration data collected at 1233 air quality monitoring stations in Beijing and the whole China from January 1, 2016 to December 31, 2017 were used to validate the effectiveness of the proposed C-LSTME model. The results show that the present model has achieved better performance than current state-of-the-art models for different time predictions at different regional scales.\},\
	language = \{en\},\
	urldate = \{2022-03-28\},\
	journal = \{Science of The Total Environment\},\
	author = \{Wen, Congcong and Liu, Shufu and Yao, Xiaojing and Peng, Ling and Li, Xiang and Hu, Yuan and Chi, Tianhe\},\
	month = mar,\
	year = \{2019\},\
	keywords = \{3D convolutional neural network (3D-CNN), Air pollutant concentration predictions, Long short-term memory neural network (LSTM NN), Long-term prediction, Spatiotemporal correlation\},\
	pages = \{1091--1099\},\
	file = \{ScienceDirect Snapshot:C\\:\\\\Users\\\\konst\\\\Zotero\\\\storage\\\\PMCUGTC6\\\\S0048969718344413.html:text/html\},\
\}\
@article\{chae_pm10_2021,\
	title = \{\{PM10\} and \{PM2\}.5 real-time prediction models using an interpolated convolutional neural network\},\
	volume = \{11\},\
	copyright = \{2021 The Author(s)\},\
	issn = \{2045-2322\},\
	url = \{https://www.nature.com/articles/s41598-021-91253-9\},\
	doi = \{10.1038/s41598-021-91253-9\},\
	abstract = \{In this paper, we propose a real-time prediction model that can respond to particulate matters (PM) in the air, which are an indication of poor air quality. The model applies interpolation to air quality and weather data and then uses a Convolutional Neural Network (CNN) to predict PM concentrations. The interpolation transforms the irregular spatial data into an equally spaced grid, which the model requires. This combination creates the interpolated CNN (ICNN) model that we use to predict PM10 and PM2.5 concentrations. The PM10 and PM2.5 evaluation results show an effective prediction performance with an R-squared higher than 0.97 and a root mean square error (RMSE) of approximately 16\\% of the standard deviation. Furthermore, both PM10 and PM2.5 prediction models forecast high concentrations with high reliability, with a probability of detection higher than 0.90 and a critical success index exceeding 0.85. The proposed ICNN prediction model achieves a high prediction performance using spatio-temporal information and presents a new direction in the prediction field.\},\
	language = \{en\},\
	number = \{1\},\
	urldate = \{2022-03-28\},\
	journal = \{Scientific Reports\},\
	author = \{Chae, Sangwon and Shin, Joonhyeok and Kwon, Sungjun and Lee, Sangmok and Kang, Sungwon and Lee, Donghyun\},\
	month = jun,\
	year = \{2021\},\
	note = \{Number: 1\
Publisher: Nature Publishing Group\},\
	keywords = \{Engineering, Environmental sciences\},\
	pages = \{11952\},\
	file = \{Full Text PDF:C\\:\\\\Users\\\\konst\\\\Zotero\\\\storage\\\\TTKCJGSD\\\\Chae et al. - 2021 - PM10 and PM2.5 real-time prediction models using a.pdf:application/pdf;Snapshot:C\\:\\\\Users\\\\konst\\\\Zotero\\\\storage\\\\FX7R2N63\\\\s41598-021-91253-9.html:text/html\},\
\}\
@misc\{emc_gfs_17-03-2022,\
	title = \{\{GFS Documentation at National Centers for Environmental Prediction\}\},\
	url = \{https://www.emc.ncep.noaa.gov/emc/pages/numerical_forecast_systems/gfs/documentation.php\},\
	howpublished = \{https://www.emc.ncep.noaa.gov/emc/pages/numerical\\_forecast\\_systems/\\\\gfs/documentation.php\},\
	urldate = \{2022-03-17\},\
	note = \{accessed 17.03.2022\}\
\}\
\
@misc\{esa_sentinel-mission-page_nodate,\
	title = \{Sentinel-\{5P\} - \{Missions\} - \{Sentinel Online\}\},\
	url = \{https://sentinel.esa.int/web/sentinel/missions/sentinel-5p\},\
	urldate = \{2022-03-20\},\
	howpublished = \{https://sentinel.esa.int/web/sentinel/missions/sentinel-5p\},\
	note = \{accessed 20.03.2022\}\
\}\
\
@misc\{esa_thematic_nodate,\
	title = \{Thematic Areas and Services - \{Sentinel-5P Mission\} - \{Sentinel Online\}\},\
	url = \{https://sentinel.esa.int/web/sentinel/missions/sentinel-5p/thematic-areas-services\},\
	urldate = \{2022-03-24\},\
	howpublished = \{https://sentinel.esa.int/web/sentinel/missions/sentinel-5p/thematic-areas-services\},\
	note = \{accessed 24.03.2022\}\
\}\
\
@misc\{cnn_pandemic-air-quality,\
	title = \{Pandemic lockdowns improved air quality in 84\\% of countries worldwide, report finds\},\
	url = \{https://www.cnn.com/2021/03/16/health/world-air-quality-report-intl-hnk-scn/index.html\},\
	abstract = \{Coronavirus lockdowns led to air quality improvements in most countries, but the level of pollutants will likely rise as governments lift restrictions and economies swing back into gear, according to a new report.\},\
	howpublished = \{https://www.cnn.com/2021/03/16/health/world-air-quality-report-intl-hnk-scn/index.html\},\
	titleaddon = \{\{CNN\}\},\
	author = \{\{CNN\}, Jessie Yeung\},\
	urldate = \{2022-03-15\},\
	note = \{accessed 15.03.2022\}\
\}\
\
@misc\{who_air-pollution,\
	title = \{9 out of 10 people worldwide breathe polluted air, but more countries are taking action\},\
	url = \{https://www.who.int/news/item/02-05-2018-9-out-of-10-people-worldwide-breathe-polluted-air-but-more-countries-are-taking-action\},\
	howpublished = \{https://www.who.int/news/item/02-05-2018-9-out-of-10-people-worldwide-breathe-polluted-air-but-more-countries-are-taking-action\},\
	abstract = \{\},\
	urldate = \{2022-03-24\},\
	langid = \{english\},\
	note = \{accessed 15.03.2022\}\
\}\
\
@misc\{iqair_empowering,\
	title = \{Empowering the World to Breathe Cleaner Air \{\\textbar\} \{IQAir\}\},\
	url = \{https://www.iqair.com/world-air-quality-report\},\
	howpublished = \{https://www.iqair.com/world-air-quality-report\},\
	urldate = \{2022-03-24\},\
	note = \{accessed 15.03.2022\}\
\}\
\
@misc\{us_epa_particulate_2016,\
	title = \{Particulate Matter (\{PM\}) Basics\},\
	url = \{https://www.epa.gov/pm-pollution/particulate-matter-pm-basics\},\
	abstract = \{Particle pollution is the term for a mixture of solid particles and liquid droplets found in the air. These include "inhalable coarse particles," with diameters between 2.5 micrometers and 10 micrometers, and "fine particles," 2.5 micrometers and smaller.\},\
	type = \{Overviews and Factsheets\},\
	author = \{\{US EPA\}\},\
	urldate = \{2022-03-24\},\
	date = \{2016-04-19\},\
	langid = \{english\},\
	howpublished = \{https://www.epa.gov/pm-pollution/particulate-matter-pm-basics\},\
	note = \{accessed 15.03.2022\}\
\}\
\
@misc\{blissair_pm25,\
	title = \{What is \{PM\}2.5 and Why You Should Care \{\\textbar\} Bliss Air\},\
	url = \{https://blissair.com/what-is-pm-2-5.htm\},\
	urldate = \{2022-03-24\},\
	howpublished = \{https://blissair.com/what-is-pm-2-5.htm\},\
	note = \{accessed 15.03.2022\}\
\}\
}